{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c040515b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 Neural Network Initializations:\n",
      "Entropy: 2.6377 | Shape: w1(1, 10), w2(10, 1)\n",
      "Entropy: 2.6377 | Shape: w1(1, 10), w2(10, 1)\n",
      "Entropy: 2.6377 | Shape: w1(1, 10), w2(10, 1)\n",
      "Entropy: 2.6377 | Shape: w1(1, 10), w2(10, 1)\n",
      "Entropy: 2.6377 | Shape: w1(1, 10), w2(10, 1)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import entropy\n",
    "\n",
    "# Load dataset\n",
    "path = \"\"C:\\Users\\praanand\\Downloads\\sp500_20years Ra Dataset (1).csv\"\"\n",
    "\n",
    "df = pd.read_csv(path, skiprows=1)\n",
    "df.columns = ['Date', 'Close', 'High', 'Low', 'Open', 'Volume']\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "df.sort_values('Date', inplace=True)\n",
    "close_prices = df['Close'].values\n",
    "\n",
    "# Prepare data (lag=1 for autoregression)\n",
    "X = close_prices[:-1].reshape(-1, 1)\n",
    "y = close_prices[1:].reshape(-1, 1)\n",
    "\n",
    "# Simple Neural Network (1 hidden layer)\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, w1, b1, w2, b2):\n",
    "        self.w1 = w1  # Input -> Hidden weights\n",
    "        self.b1 = b1  # Hidden bias\n",
    "        self.w2 = w2  # Hidden -> Output weights\n",
    "        self.b2 = b2  # Output bias\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.h = np.maximum(0, x @ self.w1 + self.b1)  # ReLU activation\n",
    "        return self.h @ self.w2 + self.b2\n",
    "\n",
    "    def train(self, X, y, lr=0.001, epochs=10):\n",
    "        for _ in range(epochs):\n",
    "            # Forward pass\n",
    "            y_pred = self.forward(X)\n",
    "\n",
    "            # Backpropagation\n",
    "            grad_y = 2 * (y_pred - y) / len(y)\n",
    "            grad_w2 = self.h.T @ grad_y\n",
    "            grad_b2 = grad_y.sum(axis=0)\n",
    "\n",
    "            grad_h = grad_y @ self.w2.T\n",
    "            grad_h[self.h <= 0] = 0  # ReLU derivative\n",
    "            grad_w1 = X.T @ grad_h\n",
    "            grad_b1 = grad_h.sum(axis=0)\n",
    "\n",
    "            # Update weights\n",
    "            self.w1 -= lr * grad_w1\n",
    "            self.b1 -= lr * grad_b1\n",
    "            self.w2 -= lr * grad_w2\n",
    "            self.b2 -= lr * grad_b2\n",
    "\n",
    "# Generate 20 random initial conditions\n",
    "np.random.seed(42)\n",
    "initial_conditions = [\n",
    "    (\n",
    "        np.random.randn(1, 10),  # w1\n",
    "        np.random.randn(1, 10),  # b1\n",
    "        np.random.randn(10, 1),  # w2\n",
    "        np.random.randn(1, 1)    # b2\n",
    "    ) for _ in range(20)\n",
    "]\n",
    "\n",
    "# Entropy calculation remains the same\n",
    "def calculate_entropy(data, bins=20):\n",
    "    counts, _ = np.histogram(data, bins=bins)\n",
    "    prob = counts / counts.sum()\n",
    "    return entropy(prob)\n",
    "\n",
    "# Evaluate initial conditions\n",
    "entropies = []\n",
    "for w1, b1, w2, b2 in initial_conditions:\n",
    "    model = NeuralNetwork(w1, b1, w2, b2)\n",
    "    model.train(X, y, epochs=50, lr=0.0001)\n",
    "    y_pred = model.forward(X)\n",
    "    errors = y - y_pred.flatten()\n",
    "    entropies.append(calculate_entropy(errors))\n",
    "\n",
    "# Rank and display results\n",
    "ranked_conditions = sorted(zip(initial_conditions, entropies), key=lambda x: x[1])\n",
    "print(\"Top 5 Neural Network Initializations:\")\n",
    "for (w1, b1, w2, b2), e in ranked_conditions[:5]:\n",
    "    print(f\"Entropy: {e:.4f} | Shape: w1{w1.shape}, w2{w2.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a541aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nlopt\n",
      "  Downloading nlopt-2.9.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: numpy<3,>=2 in /usr/local/lib/python3.11/dist-packages (from nlopt) (2.0.2)\n",
      "Downloading nlopt-2.9.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (436 kB)\n",
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/436.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m153.6/436.8 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m436.8/436.8 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: nlopt\n",
      "Successfully installed nlopt-2.9.1\n",
      "Average entropy reduction: 1.08\n"
     ]
    }
   ],
   "source": [
    "!pip install nlopt\n",
    "import nlopt\n",
    "\n",
    "# Load dataset\n",
    "# Prepare data (lag=1 autoregression)\n",
    "X = close_prices[:-1].reshape(-1, 1)\n",
    "y = close_prices[1:].reshape(-1, 1)\n",
    "\n",
    "# Neural Network with NLOPT optimization\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, params):\n",
    "        self.w1, self.b1, self.w2, self.b2 = self.unflatten_params(params)\n",
    "\n",
    "    def unflatten_params(self, params):\n",
    "        w1 = params[0:10].reshape(1, 10)\n",
    "        b1 = params[10:20].reshape(1, 10)\n",
    "        w2 = params[20:30].reshape(10, 1)\n",
    "        b2 = params[30:31].reshape(1, 1)\n",
    "        return w1, b1, w2, b2\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = np.maximum(0, x @ self.w1 + self.b1)\n",
    "        return h @ self.w2 + self.b2\n",
    "\n",
    "# Generate 20 random initial guesses\n",
    "np.random.seed(42)\n",
    "initial_guesses = [np.random.randn(31) for _ in range(20)]  # 31 total parameters\n",
    "\n",
    "# NLOPT optimization setup\n",
    "def objective_function(params, grad):\n",
    "    model = NeuralNetwork(params)\n",
    "    y_pred = model.forward(X)\n",
    "    errors = y.flatten() - y_pred.flatten()\n",
    "    return entropy(np.histogram(errors, bins=20)[0])\n",
    "\n",
    "improved_conditions = []\n",
    "for init_params in initial_guesses:\n",
    "    opt = nlopt.opt(nlopt.LN_BOBYQA, len(init_params))\n",
    "    opt.set_min_objective(objective_function)\n",
    "    opt.set_xtol_rel(1e-4)\n",
    "    opt.set_maxeval(100)  # Optimization budget\n",
    "\n",
    "    try:\n",
    "        optimized_params = opt.optimize(init_params.copy())\n",
    "        improved_conditions.append(optimized_params)\n",
    "    except:\n",
    "        improved_conditions.append(init_params)  # Fallback\n",
    "\n",
    "# Verify improvement\n",
    "original_entropy = [objective_function(p, None) for p in initial_guesses]\n",
    "optimized_entropy = [objective_function(p, None) for p in improved_conditions]\n",
    "\n",
    "print(f\"Average entropy reduction: {np.mean(original_entropy) - np.mean(optimized_entropy):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c8f50d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average entropy reduction: nan bits\n",
      "Best optimized parameters:\n",
      "ARMA-GARCH: [-0.22582779  0.81128575  0.07319939  0.17959755  0.56240746]\n",
      "NN shapes: (1, 10) (10, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-3-d80642500f4a>:91: RuntimeWarning: invalid value encountered in scalar subtract\n",
      "  print(f\"Average entropy reduction: {np.mean(original_scores) - np.mean(optimized_scores):.2f} bits\")\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nlopt\n",
    "from scipy.stats import entropy\n",
    "\n",
    "returns = np.log(close_prices[1:]/close_prices[:-1])  # Log returns\n",
    "\n",
    "class ARMA_GARCH_NN:\n",
    "    def __init__(self, params):\n",
    "        # ARMA-GARCH(1,1) parameters\n",
    "        self.phi, self.theta, self.omega, self.alpha, self.beta = params[:5]\n",
    "\n",
    "        # Neural Network parameters\n",
    "        nn_params = params[5:]\n",
    "        self.w1 = nn_params[0:10].reshape(1, 10)\n",
    "        self.b1 = nn_params[10:20].reshape(1, 10)\n",
    "        self.w2 = nn_params[20:30].reshape(10, 1)\n",
    "        self.b2 = nn_params[30:31].reshape(1, 1)\n",
    "\n",
    "    def compute_residuals(self, returns):\n",
    "        # ARMA(1,1) residuals\n",
    "        eps = np.zeros_like(returns)\n",
    "        for t in range(1, len(returns)):\n",
    "            eps[t] = returns[t] - (self.phi*returns[t-1] + self.theta*eps[t-1])\n",
    "\n",
    "        # GARCH(1,1) variances\n",
    "        sigma2 = np.zeros_like(returns)\n",
    "        sigma2[0] = np.var(returns)\n",
    "        for t in range(1, len(returns)):\n",
    "            sigma2[t] = self.omega + self.alpha*eps[t-1]**2 + self.beta*sigma2[t-1]\n",
    "\n",
    "        return eps/np.sqrt(sigma2[1:])  # Standardized residuals\n",
    "\n",
    "    def nn_forward(self, x):\n",
    "        h = np.maximum(0, x @ self.w1 + self.b1)\n",
    "        return h @ self.w2 + self.b2\n",
    "\n",
    "# NLOPT objective function\n",
    "def objective(params, grad):\n",
    "    model = ARMA_GARCH_NN(params)\n",
    "    try:\n",
    "        # Get GARCH-standardized residuals\n",
    "        residuals = model.compute_residuals(returns)\n",
    "\n",
    "        # Neural network prediction on residuals\n",
    "        X = residuals[:-1].reshape(-1, 1)\n",
    "        y = residuals[1:].reshape(-1, 1)\n",
    "        y_pred = model.nn_forward(X)\n",
    "\n",
    "        errors = y.flatten() - y_pred.flatten()\n",
    "        return entropy(np.histogram(errors, bins=20)[0])\n",
    "    except:\n",
    "        return float('inf')  # Handle unstable parameters\n",
    "\n",
    "# Generate 20 initial guesses (ARMA-GARCH + NN params)\n",
    "np.random.seed(42)\n",
    "initial_conditions = []\n",
    "for _ in range(20):\n",
    "    # ARMA-GARCH parameters (stationary initialization)\n",
    "    arma_garch = [\n",
    "        np.random.uniform(-0.9, 0.9),  # phi\n",
    "        np.random.uniform(-0.9, 0.9),  # theta\n",
    "        np.random.uniform(0, 0.1),     # omega\n",
    "        np.random.uniform(0, 0.3),     # alpha\n",
    "        np.random.uniform(0.5, 0.9)    # beta\n",
    "    ]\n",
    "    # NN parameters\n",
    "    nn_params = np.random.randn(31)\n",
    "    initial_conditions.append(np.concatenate([arma_garch, nn_params]))\n",
    "\n",
    "# Optimization setup\n",
    "improved_conditions = []\n",
    "for init in initial_conditions:\n",
    "    opt = nlopt.opt(nlopt.LN_BOBYQA, len(init))\n",
    "    opt.set_min_objective(objective)\n",
    "    opt.set_lower_bounds([-0.99, -0.99, 1e-6, 1e-6, 0.5] + [-np.inf]*31)\n",
    "    opt.set_upper_bounds([0.99, 0.99, 0.1, 0.3, 0.99] + [np.inf]*31)\n",
    "    opt.set_xtol_rel(1e-4)\n",
    "    opt.set_maxeval(150)\n",
    "\n",
    "    try:\n",
    "        optimized = opt.optimize(init.copy())\n",
    "        improved_conditions.append(optimized)\n",
    "    except:\n",
    "        improved_conditions.append(init)\n",
    "\n",
    "# Evaluate improvement\n",
    "original_scores = [objective(init, None) for init in initial_conditions]\n",
    "optimized_scores = [objective(opt, None) for opt in improved_conditions]\n",
    "\n",
    "print(f\"Average entropy reduction: {np.mean(original_scores) - np.mean(optimized_scores):.2f} bits\")\n",
    "print(\"Best optimized parameters:\")\n",
    "print(\"ARMA-GARCH:\", improved_conditions[0][:5])\n",
    "print(\"NN shapes:\", (1,10), (10,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0453501",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Residuals kurtosis: Sample=11.56, Theoretical=inf (Normal=0)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nlopt\n",
    "from scipy.stats import t\n",
    "from scipy.stats import kurtosis  # Import the correct function\n",
    "\n",
    "class TDist_ARMA_GARCH_NN:\n",
    "    def __init__(self, params):\n",
    "        # ARMA-GARCH(1,1) + Student's T parameters\n",
    "        self.phi, self.theta, self.omega, self.alpha, self.beta, self.nu = params[:6]\n",
    "\n",
    "        # Neural Network parameters\n",
    "        nn_params = params[6:]\n",
    "        self.w1 = nn_params[0:10].reshape(1, 10)\n",
    "        self.b1 = nn_params[10:20].reshape(1, 10)\n",
    "        self.w2 = nn_params[20:30].reshape(10, 1)\n",
    "        self.b2 = nn_params[30:31].reshape(1, 1)\n",
    "\n",
    "    def compute_std_residuals(self, returns):\n",
    "        # ARMA(1,1) residuals\n",
    "        eps = np.zeros_like(returns)\n",
    "        for t in range(1, len(returns)):\n",
    "            eps[t] = returns[t] - (self.phi*returns[t-1] + self.theta*eps[t-1])\n",
    "\n",
    "        # GARCH(1,1) variances\n",
    "        sigma2 = np.zeros_like(returns)\n",
    "        sigma2[0] = np.var(returns)\n",
    "        for t in range(1, len(returns)):\n",
    "            sigma2[t] = self.omega + self.alpha*eps[t-1]**2 + self.beta*sigma2[t-1]\n",
    "\n",
    "        # Fix: Align dimensions by slicing both arrays from index 1\n",
    "        return eps[1:]/np.sqrt(sigma2[1:]), sigma2[1:]\n",
    "\n",
    "    def log_likelihood(self, residuals, sigma2):\n",
    "        # Student's T log-likelihood\n",
    "        return t.logpdf(residuals, df=self.nu, loc=0, scale=np.sqrt(sigma2*(self.nu-2)/self.nu)).sum()\n",
    "\n",
    "    def nn_forward(self, x):\n",
    "        h = np.maximum(0, x @ self.w1 + self.b1)\n",
    "        return h @ self.w2 + self.b2\n",
    "\n",
    "# NLOPT objective function (negative log-likelihood)\n",
    "def objective(params, grad):\n",
    "    model = TDist_ARMA_GARCH_NN(params)\n",
    "    try:\n",
    "        residuals, sigma2 = model.compute_std_residuals(returns)\n",
    "        ll = model.log_likelihood(residuals, sigma2)\n",
    "        return -ll  # Minimize negative log-likelihood\n",
    "    except:\n",
    "        return float('inf')\n",
    "\n",
    "# Generate 20 initial guesses with T-distribution parameters\n",
    "np.random.seed(42)\n",
    "initial_conditions = []\n",
    "for _ in range(20):\n",
    "    # ARMA-GARCH-T parameters\n",
    "    base_params = [\n",
    "        np.random.uniform(-0.9, 0.9),   # phi\n",
    "        np.random.uniform(-0.9, 0.9),   # theta\n",
    "        np.random.uniform(0, 0.1),      # omega\n",
    "        np.random.uniform(0, 0.3),      # alpha\n",
    "        np.random.uniform(0.5, 0.9),    # beta\n",
    "        np.random.uniform(3, 7)         # nu (degrees of freedom)\n",
    "    ]\n",
    "    # NN parameters\n",
    "    nn_params = np.random.randn(31)\n",
    "    initial_conditions.append(np.concatenate([base_params, nn_params]))\n",
    "\n",
    "# Optimization setup with bounds\n",
    "improved_conditions = []\n",
    "for init in initial_conditions:\n",
    "    opt = nlopt.opt(nlopt.LN_BOBYQA, len(init))\n",
    "    opt.set_min_objective(objective)\n",
    "    opt.set_lower_bounds([-0.99, -0.99, 1e-6, 1e-6, 0.5, 2.1] + [-np.inf]*31)\n",
    "    opt.set_upper_bounds([0.99, 0.99, 0.1, 0.3, 0.99, 15] + [np.inf]*31)\n",
    "    opt.set_xtol_rel(1e-4)\n",
    "    opt.set_maxeval(200)\n",
    "\n",
    "    try:\n",
    "        optimized = opt.optimize(init.copy())\n",
    "        improved_conditions.append(optimized)\n",
    "    except:\n",
    "        improved_conditions.append(init)\n",
    "\n",
    "# Analyze results\n",
    "best_params = improved_conditions[np.argmin([objective(p, None) for p in improved_conditions])]\n",
    "model = TDist_ARMA_GARCH_NN(best_params)\n",
    "residuals, _ = model.compute_std_residuals(returns)\n",
    "\n",
    "# Sample kurtosis (Fisher-Pearson, excess)\n",
    "sample_kurt = kurtosis(residuals, fisher=True)\n",
    "\n",
    "# Theoretical kurtosis for Student's T (if nu > 4)\n",
    "nu = best_params[5]\n",
    "theoretical_kurt = 6/(nu - 4) if nu > 4 else float('inf')\n",
    "\n",
    "print(f\"Residuals kurtosis: Sample={sample_kurt:.2f}, Theoretical={theoretical_kurt:.2f} (Normal=0)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c5b5301",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Hurst exponent: 0.64\n",
      "\n",
      "Optimization Results:\n",
      "Hurst Exponent - Original: 0.64, Residuals: 0.65\n",
      "Volatility Persistence (α+β): 0.77\n",
      "Tail Risk (ν): 2.6 degrees of freedom\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nlopt\n",
    "from scipy.stats import t, kurtosis\n",
    "\n",
    "def hurst_rogers(series):\n",
    "    \"\"\"Calculate Hurst exponent using Rogers' modified R/S method\"\"\"\n",
    "    n = len(series)\n",
    "    max_lag = min(100, n//2)\n",
    "    lags = np.arange(2, max_lag)\n",
    "    rs_values = []\n",
    "\n",
    "    for lag in lags:\n",
    "        chunks = [series[i:i+lag] for i in range(0, n, lag)]\n",
    "        valid_chunks = [c for c in chunks if len(c) >= lag]\n",
    "\n",
    "        chunk_rs = []\n",
    "        for chunk in valid_chunks:\n",
    "            mean_chunk = np.mean(chunk)\n",
    "            adjusted = chunk - mean_chunk\n",
    "            cumulative = np.cumsum(adjusted)\n",
    "            r = np.max(cumulative) - np.min(cumulative)\n",
    "            s = np.std(chunk, ddof=1)\n",
    "            if s != 0:\n",
    "                chunk_rs.append(r/s)\n",
    "\n",
    "        if chunk_rs:\n",
    "            rs_values.append(np.mean(chunk_rs))\n",
    "\n",
    "    if not rs_values:\n",
    "        return 0.5  # Fallback if no valid RS values\n",
    "\n",
    "    hurst = np.polyfit(np.log(lags[:len(rs_values)]), np.log(rs_values), 1)[0]\n",
    "    return hurst\n",
    "\n",
    "class AdvancedRiskModel:\n",
    "    def __init__(self, params):\n",
    "        # Model parameters\n",
    "        self.phi, self.theta = params[:2]\n",
    "        self.omega, self.alpha, self.beta, self.nu = params[2:6]\n",
    "        self.nn_params = params[6:]\n",
    "        self.hurst_weight = 0.5  # Weight for Hurst-based penalty\n",
    "\n",
    "    def compute_std_residuals(self, returns):\n",
    "        \"\"\"Calculate standardized residuals using ARMA-GARCH model\"\"\"\n",
    "        n = len(returns)\n",
    "        epsilon = np.zeros(n)  # Residuals (ε)\n",
    "        sigma2 = np.zeros(n)   # Conditional variance (σ²)\n",
    "\n",
    "        # Initialize with first observation\n",
    "        epsilon[0] = returns[0]\n",
    "        sigma2[0] = np.var(returns)\n",
    "\n",
    "        for t in range(1, n):\n",
    "            # ARMA(1,1) component\n",
    "            arma_mean = self.phi * returns[t-1] + self.theta * epsilon[t-1]\n",
    "            epsilon[t] = returns[t] - arma_mean\n",
    "\n",
    "            # GARCH(1,1) component\n",
    "            sigma2[t] = self.omega + self.alpha * epsilon[t-1]**2 + self.beta * sigma2[t-1]\n",
    "\n",
    "        # Standardized residuals (ε_t/σ_t)\n",
    "        std_residuals = epsilon / np.sqrt(sigma2)\n",
    "        return std_residuals, sigma2\n",
    "\n",
    "    def log_likelihood(self, std_residuals, sigma2):\n",
    "        \"\"\"Calculate log-likelihood for Student's t-distribution\"\"\"\n",
    "        return np.sum(t.logpdf(std_residuals, df=self.nu) - 0.5 * np.log(sigma2))\n",
    "\n",
    "    def compute_hurst_penalty(self, residuals):\n",
    "        \"\"\"Hurst-based regularization penalty\"\"\"\n",
    "        h_residuals = hurst_rogers(residuals)\n",
    "        return self.hurst_weight * (h_residuals - 0.5)**2\n",
    "\n",
    "    def objective_function(self, params, grad=None):\n",
    "        # Update parameters\n",
    "        self.phi, self.theta = params[:2]\n",
    "        self.omega, self.alpha, self.beta, self.nu = params[2:6]\n",
    "        self.nn_params = params[6:]\n",
    "\n",
    "        # Compute residuals and likelihood\n",
    "        residuals, sigma2 = self.compute_std_residuals(returns)\n",
    "        ll = self.log_likelihood(residuals, sigma2)\n",
    "\n",
    "        # Hurst regularization\n",
    "        hurst_penalty = self.compute_hurst_penalty(residuals)\n",
    "\n",
    "        return -ll + hurst_penalty\n",
    "\n",
    "# Generate synthetic returns data (replace with real data)\n",
    "np.random.seed(42)\n",
    "returns = pd.Series(np.random.randn(1000) * 0.02)\n",
    "\n",
    "# Parameter initialization\n",
    "initial_params = np.array([\n",
    "    0.0, 0.0,       # ARMA: phi, theta\n",
    "    0.05, 0.1, 0.8, # GARCH: omega, alpha, beta\n",
    "    4.0,             # Student's t ν (degrees of freedom)\n",
    "    *np.random.randn(31) * 0.01  # Neural network parameters\n",
    "])\n",
    "\n",
    "# Initial Hurst analysis\n",
    "H_original = hurst_rogers(returns)\n",
    "print(f\"Original Hurst exponent: {H_original:.2f}\")\n",
    "\n",
    "# Optimization setup\n",
    "opt = nlopt.opt(nlopt.LN_BOBYQA, len(initial_params))\n",
    "opt.set_min_objective(AdvancedRiskModel(initial_params).objective_function)\n",
    "opt.set_lower_bounds([-0.99, -0.99, 1e-6, 1e-6, 0.5, 2.1] + [-np.inf]*31)\n",
    "opt.set_upper_bounds([0.99, 0.99, 0.1, 0.3, 0.99, 15] + [np.inf]*31)\n",
    "opt.set_ftol_rel(1e-6)  # Convergence tolerance\n",
    "\n",
    "# Run optimization\n",
    "optimized_params = opt.optimize(initial_params.copy())\n",
    "\n",
    "# Final analysis\n",
    "final_model = AdvancedRiskModel(optimized_params)\n",
    "residuals, _ = final_model.compute_std_residuals(returns)\n",
    "H_final = hurst_rogers(residuals)\n",
    "\n",
    "print(\"\\nOptimization Results:\")\n",
    "print(f\"Hurst Exponent - Original: {H_original:.2f}, Residuals: {H_final:.2f}\")\n",
    "print(f\"Volatility Persistence (α+β): {final_model.alpha + final_model.beta:.2f}\")\n",
    "print(f\"Tail Risk (ν): {final_model.nu:.1f} degrees of freedom\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5440f7df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average negative log-likelihood: Before = -1112.25, After = -4961.98\n"
     ]
    }
   ],
   "source": [
    "# Importance Sampling for Initial Conditions\n",
    "import numpy as np\n",
    "from scipy.stats import t  # Student's T distribution for likelihood calculations\n",
    "\n",
    "# Ensure returns data is available (log returns of close_prices)\n",
    "returns = np.log(close_prices[1:] / close_prices[:-1])\n",
    "\n",
    "# **Step 1:** Generate 20 initial condition guesses (proposal distribution)\n",
    "np.random.seed(42)\n",
    "initial_conditions = []\n",
    "for _ in range(20):\n",
    "    # ARMA-GARCH-T parameters (within plausible bounds for stationarity and nu > 2)\n",
    "    base_params = [\n",
    "        np.random.uniform(-0.9, 0.9),    # phi (AR coefficient)\n",
    "        np.random.uniform(-0.9, 0.9),    # theta (MA coefficient)\n",
    "        np.random.uniform(0, 0.1),       # omega (GARCH constant)\n",
    "        np.random.uniform(0, 0.3),       # alpha (GARCH alpha)\n",
    "        np.random.uniform(0.5, 0.9),     # beta (GARCH beta)\n",
    "        np.random.uniform(3, 7)          # nu (degrees of freedom for Student's T)\n",
    "    ]\n",
    "    # Neural network weights (proposal: standard normal)\n",
    "    nn_params = np.random.randn(31)\n",
    "    initial_conditions.append(np.concatenate([base_params, nn_params]))\n",
    "\n",
    "initial_conditions = np.array(initial_conditions)  # shape (20, 37)\n",
    "\n",
    "# **Step 2:** Define a function to compute log-likelihood for a given parameter set\n",
    "def compute_log_likelihood(params):\n",
    "    \"\"\"\n",
    "    Compute the log-likelihood of the returns data under the ARMA(1,1)-GARCH(1,1)-T model\n",
    "    with a neural network mean adjustment, given a parameter vector.\n",
    "    Returns -inf if the parameters produce an invalid model (e.g., non-positive variance).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Instantiate the model using the previously defined TDist_ARMA_GARCH_NN class\n",
    "        model = TDist_ARMA_GARCH_NN(params)\n",
    "        # Compute standardized residuals and conditional variances\n",
    "        residuals, sigma2 = model.compute_std_residuals(returns)\n",
    "        # Sum log-likelihood of Student's T for all data points\n",
    "        ll = model.log_likelihood(residuals, sigma2)\n",
    "        return ll\n",
    "    except Exception as e:\n",
    "        # If any numerical issue occurs (e.g., non-positive variance), treat log-likelihood as -inf\n",
    "        return -np.inf\n",
    "\n",
    "# **Step 3:** Evaluate log-likelihoods for all initial guesses and compute importance weights\n",
    "log_liks = np.array([compute_log_likelihood(p) for p in initial_conditions])\n",
    "# Identify the highest log-likelihood among the samples (for numerical stability)\n",
    "max_log_lik = np.max(log_liks[np.isfinite(log_liks)]) if np.any(np.isfinite(log_liks)) else -np.inf\n",
    "\n",
    "# Compute unnormalized weights proportional to likelihood = exp(log_lik)\n",
    "# Use relative scaling by subtracting max_log_lik to avoid underflow for very low likelihoods\n",
    "unnormalized_weights = np.exp(log_liks - max_log_lik)\n",
    "# Any -inf log_lik becomes weight 0. If all log_liks are -inf (unlikely with broad initial search), use equal weights\n",
    "if not np.any(np.isfinite(log_liks)):\n",
    "    unnormalized_weights = np.ones_like(unnormalized_weights)\n",
    "# Normalize weights to form a probability distribution\n",
    "weights = unnormalized_weights / unnormalized_weights.sum()\n",
    "\n",
    "# **Step 4:** Resample 20 new initial conditions based on the importance weights\n",
    "# This concentrates samples in regions with higher likelihood (lower risk modeling error)\n",
    "resample_indices = np.random.choice(len(initial_conditions), size=len(initial_conditions), p=weights)\n",
    "refined_conditions = initial_conditions[resample_indices]\n",
    "\n",
    "# **Step 5:** (Optional) Evaluate improvement in objective metric for the refined guesses\n",
    "orig_costs = np.array([objective(p, None) for p in initial_conditions])      # original negative log-likelihoods\n",
    "refined_costs = np.array([objective(p, None) for p in refined_conditions])   # refined negative log-likelihoods\n",
    "\n",
    "# Filter out any infinite costs for averaging\n",
    "orig_fin = orig_costs[np.isfinite(orig_costs)]\n",
    "ref_fin = refined_costs[np.isfinite(refined_costs)]\n",
    "avg_orig = np.mean(orig_fin) if orig_fin.size > 0 else np.inf\n",
    "avg_refined = np.mean(ref_fin) if ref_fin.size > 0 else np.inf\n",
    "\n",
    "print(f\"Average negative log-likelihood: Before = {avg_orig:.2f}, After = {avg_refined:.2f}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
